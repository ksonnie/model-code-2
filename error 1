import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import IntegerType
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PowerTransformer

# Initialize Spark session
spark = SparkSession.builder.appName("Round Columns").getOrCreate()

# Define a UDF to round up values
def custom_round(value):
    if value is not None:
        return int(value) + (1 if value - int(value) > 0.5 else 0)
    return None

# Register the UDF
round_udf = F.udf(custom_round, IntegerType())

# Function to read, process, and round data
def process_data(sheet_name, bso_name):
    # Read the sheet into a DataFrame
    data = pd.read_excel("/Volumes/navy-fip4-ya-dev/data_cleansing_dev/visualization_dev/All BSOs Dashboards.xlsx", sheet_name=sheet_name)
    
    # Convert Pandas DataFrame to Spark DataFrame
    data_spark = spark.createDataFrame(data)
    
    # Get all columns excluding the first column (date)
    all_columns = data_spark.columns
    columns_to_round = all_columns[1:]  # Skip the first column

    # Apply custom rounding to each specified column
    for column in columns_to_round:
        data_spark = data_spark.withColumn(column, round_udf(data_spark[column]))

    # Add BSO column
    data_spark = data_spark.withColumn('BSO', F.lit(bso_name))
    
    return data_spark

# Process each BSO data
ffc_data = process_data("FFC Proj", 'USFFC')
resfor_data = process_data("RESFOR Proj", 'RESFOR')
pacflt_data = process_data("PACFLT Proj", 'PACFLT')

# Specify the columns for KPI_Count_High
kpi_count_high_columns = [
    'NULOs that Remain High Priority',
    'Billing exceeds Authorization (Abnormal Unfilled Customer Orders)',
    'RBC Level- Total Negative Billed',
    'RBC Level- Total Negative Authorizations',
    'RON Level- Total Negative Reimbursable Collected',
    'Negative Liquidations - Travel (DTC: CS, CT, TO)',
    'Permanent Journal Vouchers',
    'OTA w/ Closed RON',
    'Negative Liquidations - Other',
    'OTA w/ Excluded Purchase Order (PO)',
    'Negative Liquidations - Foreign Currency',
    'Cost Transfer Allocation - "COE"',
    'ZMIL Negative Quantity / Positive Liquidation Amounts',
    'Cost Transfer Allocation - "L"',
    'Non-Labor Duplicates on SDN/ACRN/CLIN/SLIN',
    'Invalid LOA'
]

# Function to run Random Forest model on a DataFrame
def run_random_forest(data):
    data = data.dropna()  # Handle missing values
    data = data.toPandas()  # Convert to Pandas DataFrame for sklearn processing
    data['Date'] = pd.to_datetime(data['Date'])  # Ensure Date is in datetime format
    data['DateNumeric'] = data['Date'].map(pd.Timestamp.toordinal)  # Convert Date to numeric format

    # Prepare future dates for prediction (next 3 weeks)
    last_date = data['Date'].max()
    future_dates = [last_date + pd.Timedelta(weeks=i) for i in range(1, 4)]
    future_numeric = pd.Series(future_dates).map(pd.Timestamp.toordinal).to_frame(name='DateNumeric')

    # DataFrame to hold all future predictions
    future_predictions_df = pd.DataFrame({'Date': future_dates})

    for column in kpi_count_high_columns:
        if column in data.columns:
            X = data[['DateNumeric']]
            y = data[column].values

            # Apply transformation
            transformer = PowerTransformer()
            y_transformed = transformer.fit_transform(y.reshape(-1, 1)).flatten()

            # Split data
            X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.2, random_state=42)

            # Initialize Random Forest model
            model = RandomForestRegressor(random_state=42)

            # Train the model
            model.fit(X_train, y_train)

            # Predictions
            future_predictions = model.predict(future_numeric)

            # Transform back the predictions
            future_predictions = transformer.inverse_transform(future_predictions.reshape(-1, 1)).flatten()

            # Store predictions in the future predictions DataFrame
            future_predictions_df[column] = future_predictions

      # Add the BSO column to the predictions
    future_predictions_df['BSO'] = data['BSO'].iloc[0]  # Use the BSO value from the original data

    return future_predictions_df

# Run the Random Forest model for each DataFrame and collect predictions
ffc_predictions = run_random_forest(ffc_data)
resfor_predictions = run_random_forest(resfor_data)
pacflt_predictions = run_random_forest(pacflt_data)

# Combine all predictions into a single DataFrame
final_predictions_df = pd.concat([ffc_predictions, resfor_predictions, pacflt_predictions], ignore_index=True)

# Append the original data with the predictions (if needed)
combined_data = pd.concat([ffc_data.toPandas(), resfor_data.toPandas(), pacflt_data.toPandas()], ignore_index=True)
final_data_with_predictions = pd.concat([combined_data, final_predictions_df], ignore_index=True)
final_data_with_predictions['Date'] = pd.to_datetime(final_data_with_predictions['Date']).dt.strftime('%m/%d/%Y')

# Convert the final DataFrame back to a Spark DataFrame
final_data_with_predictions_spark = spark.createDataFrame(final_data_with_predictions)

# Display the Spark DataFrame
final_data_with_predictions_spark.show()
