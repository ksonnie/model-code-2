import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# File path and relevant sheet name
file_path = "/Volumes/navy-fip4-ya-dev/data_cleansing_dev/visualization_dev/All BSOs Dashboards.xlsx"
sheet_name = "FFC Proj"

# List of KPI columns to process
kpi_columns = [
    "NULOs that Remain High Priority",
    "Billing exceeds Authorization (Abnormal Unfilled Customer Orders)",
    "RBC Level- Total Negative Billed",
    "RBC Level- Total Negative Authorizations",
    # Add more KPI columns as required
]

# Load the Excel data
def load_data(file_path, sheet_name):
    data = pd.read_excel(file_path, sheet_name=sheet_name)
    data['Date'] = pd.to_datetime(data['Date'], errors='coerce')
    data.dropna(subset=['Date'], inplace=True)  # Drop rows with invalid dates
    return data

# Function to process each KPI column and append predictions
def process_kpi_columns(data, kpi_columns):
    scaler = MinMaxScaler()

    for kpi_column in kpi_columns:
        if kpi_column in data.columns:
            print(f"Processing KPI: {kpi_column}")

            # Prepare historical data for the current KPI
            kpi_data = data[['Date', kpi_column]].copy()
            kpi_data.dropna(subset=[kpi_column], inplace=True)  # Remove rows where KPI is NaN

            # Skip processing if insufficient data
            if len(kpi_data) < 2:
                print(f"Skipping {kpi_column}: insufficient data.")
                continue

            # Convert Date to numeric for modeling
            kpi_data['DateNumeric'] = kpi_data['Date'].map(pd.Timestamp.toordinal)
            X = kpi_data[['DateNumeric']]
            y = kpi_data[kpi_column]

            # Scale the numeric data
            X_scaled = scaler.fit_transform(X)

            # Initialize and train the Random Forest model
            model = RandomForestRegressor(random_state=42)
            model.fit(X_scaled, y)

            # Predict for the next 3 weeks
            last_date = kpi_data['Date'].max()
            predictions = []

            for i in range(3):
                # Calculate the next date for prediction
                next_date = last_date + pd.Timedelta(days=7)

                # Prepare input for prediction
                next_date_numeric = scaler.transform([[next_date.toordinal()]])[0][0]
                next_prediction = model.predict([[next_date_numeric]])[0]

                # Append prediction
                predictions.append({'Date': next_date, kpi_column: next_prediction})

                # Update historical data with the predicted value
                last_date = next_date
                new_row = {'Date': next_date, 'DateNumeric': next_date.toordinal(), kpi_column: next_prediction}
                kpi_data = pd.concat([kpi_data, pd.DataFrame([new_row])], ignore_index=True)

                # Retrain the model with updated data
                X = kpi_data[['DateNumeric']]
                y = kpi_data[kpi_column]
                X_scaled = scaler.fit_transform(X)
                model.fit(X_scaled, y)

            # Convert predictions to a DataFrame and append to the original data
            prediction_df = pd.DataFrame(predictions)
            data = pd.concat([data, prediction_df], ignore_index=True)

    # Sort the data by Date and reset the index
    data.sort_values(by='Date', inplace=True)
    data.reset_index(drop=True, inplace=True)
    return data

# Main execution
data = load_data(file_path, sheet_name)
final_data = process_kpi_columns(data, kpi_columns)

# Display the final DataFrame with predictions
print(final_data)
