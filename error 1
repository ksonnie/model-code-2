import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler

# File path and sheet information
file_path = "/Volumes/navy-fip4-ya-dev/data_cleansing_dev/visualization_dev/All BSOs Dashboards.xlsx"
bso_sheets = {
    'USFFC': 'FFC Proj'
}

# Define KPI columns to process
kpi_columns = [
    "NULOs that Remain High Priority",
    "Billing exceeds Authorization (Abnormal Unfilled Customer Orders)",
    "RBC Level- Total Negative Billed",
    "RBC Level- Total Negative Authorizations",
]

# Function to preprocess, predict, and append predictions to the original data
def process_and_predict(sheet_name, kpi_columns):
    # Load the data
    data = pd.read_excel(file_path, sheet_name=sheet_name)
    data['Date'] = pd.to_datetime(data['Date'], errors='coerce')
    data = data.dropna(subset=['Date'])  # Drop rows with invalid dates

    # Convert Date to numeric format
    data['DateNumeric'] = data['Date'].map(pd.Timestamp.toordinal)

    # Initialize the MinMaxScaler
    scaler = MinMaxScaler()

    # DataFrame to hold results
    result_data = data.copy()

    for kpi_column in kpi_columns:
        if kpi_column in data.columns:
            print(f"Processing KPI: {kpi_column}")

            # Prepare data for model training
            X = data[['DateNumeric']]
            y = data[kpi_column]

            # Validate the target data
            if y.isnull().sum() > 0 or y.nunique() <= 1:
                print(f"Skipping {kpi_column}: insufficient or constant data.")
                continue

            # Scale the input data
            X_scaled = scaler.fit_transform(X)

            # Initialize the model
            model = RandomForestRegressor(random_state=42)

            # Train the model on historical data
            model.fit(X_scaled, y)

            # Predict the next 3 weeks dynamically updating historical data
            last_date = data['Date'].max()
            for i in range(3):
                # Calculate the next prediction date
                next_date = last_date + pd.Timedelta(days=7)

                # Prepare the next date for prediction
                next_date_numeric = scaler.transform([[next_date.toordinal()]])[0][0]
                next_prediction = model.predict([[next_date_numeric]])[0]

                # Append the prediction to the historical data
                new_row = {'Date': next_date, 'DateNumeric': next_date.toordinal(), kpi_column: next_prediction}
                result_data = pd.concat([result_data, pd.DataFrame([new_row])], ignore_index=True)

                # Update historical data for the next iteration
                X_scaled = np.append(X_scaled, [[next_date_numeric]], axis=0)
                y = np.append(y, next_prediction)
                model.fit(X_scaled, y)  # Retrain the model with updated data

                # Update the last date
                last_date = next_date

    # Sort the final DataFrame by date
    result_data = result_data.sort_values(by='Date').reset_index(drop=True)

    # Drop the DateNumeric column before returning the result
    result_data = result_data.drop(columns=['DateNumeric'])
    return result_data

# Process each sheet and generate predictions
final_results = pd.DataFrame()
for bso, sheet_name in bso_sheets.items():
    print(f"Processing BSO: {bso}")
    predictions = process_and_predict(sheet_name, kpi_columns)
    predictions['BSO'] = bso  # Add the BSO column to the results
    final_results = pd.concat([final_results, predictions], ignore_index=True)

# Display the final DataFrame
print(final_results)
