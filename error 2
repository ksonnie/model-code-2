import os
import pandas as pd
from pyspark.sql import SparkSession
from datetime import datetime

# Create a Spark session
spark = SparkSession.builder.appName("ProcessExcelFiles").getOrCreate()

# Define the path to the volume
volume_path = "/Volumes/navy-fip4-prod/dev_data/dev_deaddrop/"  # Direct path without /dbfs/

# Initialize an empty list to store processed data
processed_data = []

# List all files in the directory
files = os.listdir(volume_path)

# Get the current date in YYYYMMDD format
current_date = datetime.now().strftime("%Y%m%d")

# Iterate through each file
for file_name in files:
    file_path = os.path.join(volume_path, file_name)
    
    # Strictly match the file naming format: tblKPI_AAP_Old_YYYYMMDD KPI.xlsx
    if file_name.startswith("tblKPI") and "_Old_" in file_name and file_name.endswith("KPI.xlsx"):
        # Extract the KPI and date from the filename
        parts = file_name.split("_")
        if len(parts) >= 4:
            kpi = parts[1]  # Extract the KPI
            date_part = parts[3].split(" ")[0]  # Extract the date (YYYYMMDD)
        
            # Read the Excel file
            xls = pd.ExcelFile(file_path)  # Use the direct path for pd.ExcelFile
            
            # Determine the sheet name (it should match the first three parts of the file name)
            sheet_name = "_".join(parts[:3])
            
            if sheet_name in xls.sheet_names:
                # Load the sheet into a DataFrame
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                
                # Dynamically identify the ABS column (e.g., 'ABS_AAP_Amount', 'ABS_XYZ_Amount')
                abs_column = [col for col in df.columns if col.startswith("ABS_") and col.endswith("_Amount")]
                if abs_column:
                    abs_column = abs_column[0]  # Get the first match
                    
                    # Filter rows where 'CorrectedRecord?' is TRUE
                    filtered_df = df[df["CorrectedRecord?"] == True]
                    
                    # Calculate the cumulative sum of the identified ABS column
                    filtered_df["Cumulative_ABS_Amount"] = filtered_df[abs_column].cumsum()
                    
                    # Add the date and KPI columns
                    filtered_df["Date"] = date_part
                    filtered_df["KPI"] = kpi
                    
                    # Select relevant columns
                    result_df = filtered_df[["Date", "KPI", "CorrectedRecord?", "Cumulative_ABS_Amount"]]
                    
                    # Append to the list
                    processed_data.append(result_df)

# Combine all processed data into a single DataFrame
final_df = pd.concat(processed_data, ignore_index=True)

# Convert the pandas DataFrame to a Spark DataFrame
final_spark_df = spark.createDataFrame(final_df)

# Display the final DataFrame
final_spark_df.show()

# Define the output path with dynamic file name
output_path = f"/Volumes/navy-fip4-prod/dev_data/dev_deaddrop/output/result_{current_date}.parquet"

# Save the result, overriding if the file already exists for the same date
final_spark_df.write.mode("overwrite").format("parquet").save(output_path)
